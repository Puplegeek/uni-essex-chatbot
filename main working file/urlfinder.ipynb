{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf23bd3-fb8f-4dbf-aa78-66efaee34504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 8317 URLs.\n",
      "Saved main data to 'essex_content_sitemap_2nd_Prefix.csv'\n",
      "✅ Saved 39 URLs to essex_urls_about.csv\n",
      "✅ Saved 38 URLs to essex_urls_alumni.csv\n",
      "✅ Saved 2 URLs to essex_urls_apprenticeships.csv\n",
      "✅ Saved 5 URLs to essex_urls_arena.csv\n",
      "✅ Saved 13 URLs to essex_urls_blog.csv\n",
      "✅ Saved 45 URLs to essex_urls_business.csv\n",
      "✅ Saved 209 URLs to essex_urls_centres-and-institutes.csv\n",
      "✅ Saved 6 URLs to essex_urls_china.csv\n",
      "✅ Saved 6 URLs to essex_urls_choir.csv\n",
      "✅ Saved 7 URLs to essex_urls_clearing.csv\n",
      "✅ Saved 562 URLs to essex_urls_departments.csv\n",
      "✅ Saved 14 URLs to essex_urls_disclaimer.csv\n",
      "✅ Saved 9 URLs to essex_urls_donate.csv\n",
      "✅ Saved 15 URLs to essex_urls_event-series.csv\n",
      "✅ Saved 30 URLs to essex_urls_events.csv\n",
      "✅ Saved 1 URLs to essex_urls_fees-and-funding.csv\n",
      "✅ Saved 12 URLs to essex_urls_global.csv\n",
      "✅ Saved 49 URLs to essex_urls_governance-and-strategy.csv\n",
      "✅ Saved 14 URLs to essex_urls_graduation.csv\n",
      "✅ Saved 108 URLs to essex_urls_international.csv\n",
      "✅ Saved 16 URLs to essex_urls_jobs.csv\n",
      "✅ Saved 86 URLs to essex_urls_life.csv\n",
      "✅ Saved 2538 URLs to essex_urls_news.csv\n",
      "✅ Saved 2014 URLs to essex_urls_people.csv\n",
      "✅ Saved 62 URLs to essex_urls_postgraduate.csv\n",
      "✅ Saved 78 URLs to essex_urls_research.csv\n",
      "✅ Saved 363 URLs to essex_urls_research-projects.csv\n",
      "✅ Saved 60 URLs to essex_urls_scholarships.csv\n",
      "✅ Saved 44 URLs to essex_urls_schools-and-colleges.csv\n",
      "✅ Saved 91 URLs to essex_urls_short-courses.csv\n",
      "✅ Saved 55 URLs to essex_urls_sport.csv\n",
      "✅ Saved 1 URLs to essex_urls_sport-homepage-test.csv\n",
      "✅ Saved 936 URLs to essex_urls_staff.csv\n",
      "✅ Saved 677 URLs to essex_urls_student.csv\n",
      "✅ Saved 25 URLs to essex_urls_study-abroad.csv\n",
      "✅ Saved 1 URLs to essex_urls_study-online.csv\n",
      "✅ Saved 1 URLs to essex_urls_subjects.csv\n",
      "✅ Saved 12 URLs to essex_urls_sustainability.csv\n",
      "✅ Saved 20 URLs to essex_urls_test.csv\n",
      "✅ Saved 21 URLs to essex_urls_undergraduate.csv\n",
      "✅ Saved 11 URLs to essex_urls_visit-us.csv\n",
      "✅ Saved 17 URLs to essex_urls_welcome.csv\n",
      "✅ Saved 4 URLs to essex_urls_wivenhoe-park.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sitemap URL\n",
    "sitemap_url = 'https://www.essex.ac.uk/content.xml'\n",
    "\n",
    "# Fetch and decode\n",
    "response = requests.get(sitemap_url)\n",
    "content = response.content.decode('utf-8', errors='replace')\n",
    "\n",
    "# Parse XML\n",
    "try:\n",
    "    root = ET.fromstring(content)\n",
    "    ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "\n",
    "    # Extract URLs and lastmod\n",
    "    data = []\n",
    "    for url_elem in root.findall('ns:url', ns):\n",
    "        loc = url_elem.find('ns:loc', ns)\n",
    "        lastmod = url_elem.find('ns:lastmod', ns)\n",
    "        data.append({\n",
    "            'URL': loc.text if loc is not None else '',\n",
    "            'Last Modified': lastmod.text if lastmod is not None else ''\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"✅ Extracted {len(df)} URLs.\")\n",
    "    \n",
    "    # Extract the second path segment after the domain\n",
    "    df['Second Prefix'] = df['URL'].apply(lambda url: url.replace('https://www.essex.ac.uk/', '').split('/')[0])\n",
    "    \n",
    "    # Save the main CSV file with all data and new name\n",
    "    main_filename = \"essex_content_sitemap_2nd_Prefix.csv\"\n",
    "    df.to_csv(main_filename, index=False)\n",
    "    print(f\"Saved main data to '{main_filename}'\")\n",
    "    \n",
    "    # Group by 'Second Prefix' and save each group to a CSV\n",
    "    for prefix, group_df in df.groupby('Second Prefix'):\n",
    "        # Sanitize prefix to create a valid filename\n",
    "        safe_prefix = re.sub(r'[^a-zA-Z0-9_-]', '_', prefix)\n",
    "        filename = f\"essex_urls_{safe_prefix}.csv\"\n",
    "        group_df.to_csv(filename, index=False)\n",
    "        print(f\"✅ Saved {len(group_df)} URLs to {filename}\")\n",
    "\n",
    "except ET.ParseError as e:\n",
    "    print(\"❌ XML Parse Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f510f309-4d8c-424a-8d82-41b49bcd2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hierarchical CSV created with columns:\n",
      "['Last Modified', 'URL', 'Level_1', 'Level_2', 'Level_3', 'Level_4', 'Level_5', 'Level_6', 'Level_7', 'Level_8']\n",
      "\n",
      "Sample output:\n",
      "  Last Modified                                           URL   Level_1  \\\n",
      "0    2022-11-07     https://www.essex.ac.uk/research/showcase  research   \n",
      "1    2023-04-13         https://www.essex.ac.uk/blog/post-map      blog   \n",
      "2    2023-11-10  https://www.essex.ac.uk/life/loughton-campus      life   \n",
      "\n",
      "           Level_2 Level_3 Level_4 Level_5 Level_6 Level_7 Level_8  \n",
      "0         showcase    <NA>    <NA>    <NA>    <NA>    <NA>    <NA>  \n",
      "1         post-map    <NA>    <NA>    <NA>    <NA>    <NA>    <NA>  \n",
      "2  loughton-campus    <NA>    <NA>    <NA>    <NA>    <NA>    <NA>  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Read existing CSV\n",
    "df = pd.read_csv(\"essex_content_sitemap_2nd_Prefix.csv\")\n",
    "\n",
    "# Extract path segments from URLs\n",
    "def get_hierarchy(url):\n",
    "    parsed = urlparse(url)\n",
    "    return [seg for seg in parsed.path.split('/') if seg]\n",
    "\n",
    "df['Path_Segments'] = df['URL'].apply(get_hierarchy)\n",
    "\n",
    "# Determine maximum depth needed\n",
    "max_depth = df['Path_Segments'].apply(len).max()\n",
    "\n",
    "# Create hierarchical columns\n",
    "for i in range(max_depth):\n",
    "    df[f'Level_{i+1}'] = df['Path_Segments'].apply(\n",
    "        lambda x: x[i] if i < len(x) else pd.NA\n",
    "    )\n",
    "\n",
    "# Create final dataframe with desired columns\n",
    "hierarchy_df = df[['Last Modified', 'URL'] + [f'Level_{i+1}' for i in range(max_depth)]]\n",
    "\n",
    "# Save to new CSV\n",
    "hierarchy_df.to_csv(\"essex_url_hierarchy_with_dates.csv\", index=False)\n",
    "\n",
    "print(\"✅ Hierarchical CSV created with columns:\")\n",
    "print(hierarchy_df.columns.tolist())\n",
    "print(f\"\\nSample output:\\n{hierarchy_df.head(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0401b1e-4409-4ab6-bc77-c7beac44075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Level_1 categories: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the CSV file (replace 'essex_url_hierarchy_with_dates.csv' with your file path)\n",
    "df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "\n",
    "# Extract unique Level_1 values and count them\n",
    "unique_level1 = df['Level_1'].nunique()\n",
    "\n",
    "# Alternatively, get the list of unique Level_1 categories\n",
    "# unique_level1_list = df['Level_1'].dropna().unique()\n",
    "\n",
    "print(f\"Number of unique Level_1 categories: {unique_level1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fc6e65-f5dc-44f0-b398-fac0fde67e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL                                                | Exists | Status     | Final URL\n",
      "------------------------------------------------------------------------------------------\n",
      "https://www.essex.ac.uk/research                   | True   | 200        | https://www.essex.ac.uk/research\n",
      "https://www.essex.ac.uk/blog                       | True   | 200        | https://www.essex.ac.uk/blog\n",
      "https://www.essex.ac.uk/life                       | True   | 200        | https://www.essex.ac.uk/life\n",
      "https://www.essex.ac.uk/student                    | True   | 200        | https://www.essex.ac.uk/student\n",
      "https://www.essex.ac.uk/staff                      | True   | 200        | https://www.essex.ac.uk/staff\n",
      "https://www.essex.ac.uk/centres-and-institutes     | True   | 200        | https://www.essex.ac.uk/centres-and-institutes\n",
      "https://www.essex.ac.uk/sport                      | True   | 200        | https://www.essex.ac.uk/sport\n",
      "https://www.essex.ac.uk/about                      | True   | 200        | https://www.essex.ac.uk/about\n",
      "https://www.essex.ac.uk/test                       | False  | 500        | https://www.essex.ac.uk/test\n",
      "https://www.essex.ac.uk/departments                | True   | 200        | https://www.essex.ac.uk/departments\n",
      "https://www.essex.ac.uk/governance-and-strategy    | True   | 200        | https://www.essex.ac.uk/governance-and-strategy\n",
      "https://www.essex.ac.uk/postgraduate               | True   | 200        | https://www.essex.ac.uk/postgraduate\n",
      "https://www.essex.ac.uk/wivenhoe-park              | True   | 200        | https://www.essex.ac.uk/wivenhoe-park\n",
      "https://www.essex.ac.uk/event-series               | True   | 200        | https://www.essex.ac.uk/event-series\n",
      "https://www.essex.ac.uk/jobs                       | True   | 200        | https://www.essex.ac.uk/jobs\n",
      "https://www.essex.ac.uk/global                     | True   | 200        | https://www.essex.ac.uk/global\n",
      "https://www.essex.ac.uk/business                   | True   | 200        | https://www.essex.ac.uk/business\n",
      "https://www.essex.ac.uk/schools-and-colleges       | True   | 200        | https://www.essex.ac.uk/schools-and-colleges\n",
      "https://www.essex.ac.uk/alumni                     | True   | 200        | https://www.essex.ac.uk/alumni\n",
      "https://www.essex.ac.uk/sustainability             | True   | 200        | https://www.essex.ac.uk/sustainability\n",
      "https://www.essex.ac.uk/study-online               | True   | 200        | https://www.essex.ac.uk/study-online\n",
      "https://www.essex.ac.uk/choir                      | True   | 200        | https://www.essex.ac.uk/choir\n",
      "https://www.essex.ac.uk/visit-us                   | True   | 200        | https://www.essex.ac.uk/visit-us\n",
      "https://www.essex.ac.uk/china                      | True   | 200        | https://www.essex.ac.uk/china\n",
      "https://www.essex.ac.uk/undergraduate              | True   | 200        | https://www.essex.ac.uk/undergraduate\n",
      "https://www.essex.ac.uk/donate                     | True   | 200        | https://www.essex.ac.uk/donate\n",
      "https://www.essex.ac.uk/apprenticeships            | True   | 200        | https://www.essex.ac.uk/apprenticeships\n",
      "https://www.essex.ac.uk/study-abroad               | True   | 200        | https://www.essex.ac.uk/study-abroad\n",
      "https://www.essex.ac.uk/graduation                 | True   | 200        | https://www.essex.ac.uk/graduation\n",
      "https://www.essex.ac.uk/scholarships               | True   | 200        | https://www.essex.ac.uk/scholarships\n",
      "https://www.essex.ac.uk/welcome                    | True   | 200        | https://www.essex.ac.uk/welcome\n",
      "https://www.essex.ac.uk/international              | True   | 200        | https://www.essex.ac.uk/international\n",
      "https://www.essex.ac.uk/short-courses              | True   | 200        | https://www.essex.ac.uk/short-courses\n",
      "https://www.essex.ac.uk/events                     | True   | 200        | https://www.essex.ac.uk/events\n",
      "https://www.essex.ac.uk/research-projects          | True   | 200        | https://www.essex.ac.uk/research-projects\n",
      "https://www.essex.ac.uk/disclaimer                 | True   | 200        | https://www.essex.ac.uk/disclaimer\n",
      "https://www.essex.ac.uk/sport-homepage-test        | True   | 200        | https://www.essex.ac.uk/sport-homepage-test\n",
      "https://www.essex.ac.uk/clearing                   | True   | 200        | https://www.essex.ac.uk/clearing\n",
      "https://www.essex.ac.uk/arena                      | True   | 200        | https://www.essex.ac.uk/arena\n",
      "https://www.essex.ac.uk/fees-and-funding           | True   | 200        | https://www.essex.ac.uk/fees-and-funding\n",
      "https://www.essex.ac.uk/subjects                   | True   | 200        | https://www.essex.ac.uk/subjects\n",
      "https://www.essex.ac.uk/news                       | True   | 200        | https://www.essex.ac.uk/news\n",
      "https://www.essex.ac.uk/people                     | True   | 200        | https://www.essex.ac.uk/people\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "# Load the CSV and get unique Level_1 categories\n",
    "df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "unique_level1 = df['Level_1'].dropna().unique().tolist()\n",
    "\n",
    "base_url = \"https://www.essex.ac.uk/\"\n",
    "results = {}\n",
    "\n",
    "# Configure headers to mimic a real browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "for category in unique_level1:\n",
    "    url = f\"{base_url}{category}\"\n",
    "    \n",
    "    try:\n",
    "        # Use HEAD first to check existence without downloading content\n",
    "        response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)\n",
    "        status = response.status_code\n",
    "        \n",
    "        # Consider 2xx and 3xx status codes as valid\n",
    "        if 200 <= status < 400:\n",
    "            results[url] = {\n",
    "                'exists': True,\n",
    "                'status_code': status,\n",
    "                'final_url': response.url  # Show final URL after redirects\n",
    "            }\n",
    "        else:\n",
    "            results[url] = {\n",
    "                'exists': False,\n",
    "                'status_code': status,\n",
    "                'final_url': url\n",
    "            }\n",
    "            \n",
    "    except RequestException as e:\n",
    "        results[url] = {\n",
    "            'exists': False,\n",
    "            'status_code': str(e),\n",
    "            'final_url': url\n",
    "        }\n",
    "    \n",
    "    # Be polite - add delay between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print results\n",
    "print(f\"{'URL':<50} | {'Exists':<6} | {'Status':<10} | {'Final URL'}\")\n",
    "print(\"-\" * 90)\n",
    "for url, data in results.items():\n",
    "    print(f\"{url:<50} | {str(data['exists']):<6} | {str(data['status_code']):<10} | {data['final_url']}\")\n",
    "\n",
    "# Optional: Save to CSV\n",
    "pd.DataFrame.from_dict(results, orient='index').to_csv('url_validation_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eefd856-4e3b-4d70-a256-c3dcc403636e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 43 URLs...\n",
      "\n",
      "Checked 1/43: https://www.essex.ac.uk/research => 200\n",
      "Checked 2/43: https://www.essex.ac.uk/blog => 200\n",
      "Checked 3/43: https://www.essex.ac.uk/life => 200\n",
      "Checked 4/43: https://www.essex.ac.uk/student => 200\n",
      "Checked 5/43: https://www.essex.ac.uk/staff => 200\n",
      "Checked 6/43: https://www.essex.ac.uk/centres-and-institutes => 200\n",
      "Checked 7/43: https://www.essex.ac.uk/sport => 200\n",
      "Checked 8/43: https://www.essex.ac.uk/about => 200\n",
      "Checked 9/43: https://www.essex.ac.uk/test => Connection Failed\n",
      "Checked 10/43: https://www.essex.ac.uk/departments => 200\n",
      "Checked 11/43: https://www.essex.ac.uk/governance-and-strategy => 200\n",
      "Checked 12/43: https://www.essex.ac.uk/postgraduate => 200\n",
      "Checked 13/43: https://www.essex.ac.uk/wivenhoe-park => 200\n",
      "Checked 14/43: https://www.essex.ac.uk/event-series => 200\n",
      "Checked 15/43: https://www.essex.ac.uk/jobs => 200\n",
      "Checked 16/43: https://www.essex.ac.uk/global => 200\n",
      "Checked 17/43: https://www.essex.ac.uk/business => 200\n",
      "Checked 18/43: https://www.essex.ac.uk/schools-and-colleges => 200\n",
      "Checked 19/43: https://www.essex.ac.uk/alumni => 200\n",
      "Checked 20/43: https://www.essex.ac.uk/sustainability => 200\n",
      "Checked 21/43: https://www.essex.ac.uk/study-online => 200\n",
      "Checked 22/43: https://www.essex.ac.uk/choir => 200\n",
      "Checked 23/43: https://www.essex.ac.uk/visit-us => 200\n",
      "Checked 24/43: https://www.essex.ac.uk/china => 200\n",
      "Checked 25/43: https://www.essex.ac.uk/undergraduate => 200\n",
      "Checked 26/43: https://www.essex.ac.uk/donate => 200\n",
      "Checked 27/43: https://www.essex.ac.uk/apprenticeships => 200\n",
      "Checked 28/43: https://www.essex.ac.uk/study-abroad => 200\n",
      "Checked 29/43: https://www.essex.ac.uk/graduation => 200\n",
      "Checked 30/43: https://www.essex.ac.uk/scholarships => 200\n",
      "Checked 31/43: https://www.essex.ac.uk/welcome => 200\n",
      "Checked 32/43: https://www.essex.ac.uk/international => 200\n",
      "Checked 33/43: https://www.essex.ac.uk/short-courses => 200\n",
      "Checked 34/43: https://www.essex.ac.uk/events => 200\n",
      "Checked 35/43: https://www.essex.ac.uk/research-projects => 200\n",
      "Checked 36/43: https://www.essex.ac.uk/disclaimer => 200\n",
      "Checked 37/43: https://www.essex.ac.uk/sport-homepage-test => 200\n",
      "Checked 38/43: https://www.essex.ac.uk/clearing => 200\n",
      "Checked 39/43: https://www.essex.ac.uk/arena => 200\n",
      "Checked 40/43: https://www.essex.ac.uk/fees-and-funding => 200\n",
      "Checked 41/43: https://www.essex.ac.uk/subjects => 200\n",
      "Checked 42/43: https://www.essex.ac.uk/news => 200\n",
      "Checked 43/43: https://www.essex.ac.uk/people => 200\n",
      "\n",
      "Results saved to url_validation_report.csv\n",
      "\n",
      "Validation Summary:\n",
      "Valid URLs: 42/43\n",
      "Invalid URLs: 1\n",
      "\n",
      "Detailed Status Codes:\n",
      "status_code\n",
      "200                  42\n",
      "Connection Failed     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "def check_url(url):\n",
    "    \"\"\"Check if a URL exists with fallback from HEAD to GET requests\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Try HEAD request first\n",
    "        response = requests.head(url, headers=headers, allow_redirects=True, timeout=10)\n",
    "        \n",
    "        # If HEAD not allowed, fallback to GET\n",
    "        if response.status_code == 405:\n",
    "            response = requests.get(url, headers=headers, allow_redirects=True, timeout=10)\n",
    "            \n",
    "        return response\n",
    "    \n",
    "    except RequestException as e:\n",
    "        return None\n",
    "\n",
    "def validate_level1_urls(csv_path, output_file='validation_results.csv'):\n",
    "    \"\"\"Main validation function\"\"\"\n",
    "    # Load CSV data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Get unique Level_1 categories\n",
    "    unique_level1 = df['Level_1'].dropna().unique().tolist()\n",
    "    \n",
    "    base_url = \"https://www.essex.ac.uk/\"\n",
    "    results = []\n",
    "\n",
    "    print(f\"Checking {len(unique_level1)} URLs...\\n\")\n",
    "    \n",
    "    for idx, category in enumerate(unique_level1, 1):\n",
    "        url = f\"{base_url}{category.strip().lower()}\"\n",
    "        result = {'url': url, 'exists': False, 'status_code': None, 'final_url': None}\n",
    "        \n",
    "        # Get HTTP response\n",
    "        response = check_url(url)\n",
    "        \n",
    "        if response:\n",
    "            result.update({\n",
    "                'exists': 200 <= response.status_code < 400,\n",
    "                'status_code': response.status_code,\n",
    "                'final_url': response.url\n",
    "            })\n",
    "        else:\n",
    "            result['status_code'] = 'Connection Failed'\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress update\n",
    "        print(f\"Checked {idx}/{len(unique_level1)}: {url} => {result['status_code']}\")\n",
    "        \n",
    "        # Be polite - add delay\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nResults saved to {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run validation\n",
    "    results = validate_level1_urls(\n",
    "        csv_path='essex_url_hierarchy_with_dates.csv',\n",
    "        output_file='url_validation_report.csv'\n",
    "    )\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    print(f\"Valid URLs: {results['exists'].sum()}/{len(results)}\")\n",
    "    print(f\"Invalid URLs: {len(results) - results['exists'].sum()}\")\n",
    "    print(\"\\nDetailed Status Codes:\")\n",
    "    print(results['status_code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a770b18a-bef9-4106-9c4c-e218d841f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated CSV report:\n",
      "- Original entries: 8315\n",
      "- New valid URLs added: 3\n",
      "- Final total entries: 8318\n",
      "- Duplicates removed: -3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import time\n",
    "\n",
    "def check_url(url):\n",
    "    \"\"\"Verify URL exists with browser-like headers\"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.head(url, headers=headers, \n",
    "                               allow_redirects=True, timeout=10)\n",
    "        return response.status_code == 200\n",
    "    except RequestException:\n",
    "        return False\n",
    "\n",
    "# 1. Load original data\n",
    "df = pd.read_csv(\"essex_url_hierarchy_with_dates.csv\")\n",
    "\n",
    "# 2. Get unique Level_1 categories\n",
    "base_url = \"https://www.essex.ac.uk/\"\n",
    "level1_categories = df['Level_1'].dropna().unique()\n",
    "\n",
    "# 3. Find missing valid URLs\n",
    "new_urls = []\n",
    "for category in level1_categories:\n",
    "    url = f\"{base_url}{category.strip().lower()}\"\n",
    "    \n",
    "    # Skip existing URLs\n",
    "    if url in df['URL'].values:\n",
    "        continue\n",
    "        \n",
    "    # Validate new URL\n",
    "    if check_url(url):\n",
    "        new_urls.append({\n",
    "            'URL': url,\n",
    "            'Level_1': category,\n",
    "            'Last Modified': pd.Timestamp.now().strftime('%Y-%m-%d')\n",
    "        })\n",
    "    time.sleep(0.5)  # Rate limiting\n",
    "\n",
    "# 4. Add new URLs and clean data\n",
    "if new_urls:\n",
    "    new_df = pd.DataFrame(new_urls)\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# 5. Remove duplicates (keep first occurrence)\n",
    "df = df.drop_duplicates(subset=['URL'], keep='first')\n",
    "\n",
    "# 6. Save updated file (overwrite original)\n",
    "df.to_csv(\"essex_url_hierarchy_with_dates.csv\", index=False)\n",
    "\n",
    "print(f\"\"\"\n",
    "Updated CSV report:\n",
    "- Original entries: {len(df) - len(new_urls)}\n",
    "- New valid URLs added: {len(new_urls)}\n",
    "- Final total entries: {len(df)}\n",
    "- Duplicates removed: {(len(df) - len(new_urls)) - (len(df) - df.duplicated(subset=['URL']).sum())}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c1d07d3-528a-47a4-913c-d9629fe864b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 60 URLs. Total unique URLs: 8378\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read source file with 'Subject URL' column\n",
    "source_df = pd.read_csv('essex_subject_links_selenium.csv')\n",
    "source_df.rename(columns={'Subject URL': 'URL'}, inplace=True)\n",
    "\n",
    "# Read target file (create empty DataFrame if it doesn't exist)\n",
    "try:\n",
    "    target_df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "except FileNotFoundError:\n",
    "    target_df = pd.DataFrame(columns=['URL'])  # Assumes target has at least 'URL' column\n",
    "\n",
    "# Combine and deduplicate\n",
    "combined_df = pd.concat([target_df, source_df[['URL']]], ignore_index=True)\n",
    "combined_df = combined_df.drop_duplicates(subset=['URL'], keep='first')\n",
    "\n",
    "# Save updated file\n",
    "combined_df.to_csv('essex_url_hierarchy_with_dates.csv', index=False)\n",
    "\n",
    "print(f\"Added {len(source_df)} URLs. Total unique URLs: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2064e130-b290-4a2b-87c9-75f8a60493ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 745 URLs. Total unique URLs: 9063\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_urls():\n",
    "    # Read subject URLs\n",
    "    subjects_df = pd.read_csv('essex_subject_links_selenium.csv')\n",
    "    subjects_df.rename(columns={'Subject URL': 'URL'}, inplace=True)\n",
    "    subjects_urls = subjects_df[['URL']]\n",
    "\n",
    "    # Read course URLs\n",
    "    courses_df = pd.read_csv('essex_courses_simplified.csv')\n",
    "    courses_urls = courses_df[['course_url']].rename(columns={'course_url': 'URL'})\n",
    "\n",
    "    # Combine all new URLs\n",
    "    new_urls = pd.concat([subjects_urls, courses_urls], ignore_index=True)\n",
    "\n",
    "    # Read target file (create if missing)\n",
    "    try:\n",
    "        target_df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "    except FileNotFoundError:\n",
    "        target_df = pd.DataFrame(columns=['URL'])\n",
    "\n",
    "    # Merge and deduplicate\n",
    "    combined_df = pd.concat([target_df, new_urls], ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset=['URL'], keep='first')\n",
    "\n",
    "    # Save results\n",
    "    combined_df.to_csv('essex_url_hierarchy_with_dates.csv', index=False)\n",
    "    print(f\"Added {len(new_urls)} URLs. Total unique URLs: {len(combined_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f800c012-d275-45fe-9e60-bf810582ad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL hierarchy levels updated successfully!\n",
      "Sample entry:\n",
      "Last Modified                                   2022-11-07\n",
      "URL              https://www.essex.ac.uk/research/showcase\n",
      "Level_1                                           research\n",
      "Level_2                                           showcase\n",
      "Level_3                                                NaN\n",
      "Level_4                                                NaN\n",
      "Level_5                                                NaN\n",
      "Level_6                                                NaN\n",
      "Level_7                                                NaN\n",
      "Level_8                                                NaN\n",
      "level_1                                           research\n",
      "level_2                                           showcase\n",
      "level_3                                               None\n",
      "level_4                                               None\n",
      "level_5                                               None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def extract_url_hierarchy(url):\n",
    "    \"\"\"Extract hierarchical components from Essex URLs\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    path_parts = [p for p in parsed.path.split('/') if p]  # Split and remove empty strings\n",
    "    \n",
    "    hierarchy = {\n",
    "        'level_1': None,\n",
    "        'level_2': None,\n",
    "        'level_3': None,\n",
    "        'level_4': None,\n",
    "        'level_5': None\n",
    "    }\n",
    "    \n",
    "    # Assign parts to hierarchy levels starting from first path component\n",
    "    for i, part in enumerate(path_parts[:5], start=1):\n",
    "        hierarchy[f'level_{i}'] = part\n",
    "            \n",
    "    return hierarchy\n",
    "\n",
    "def update_hierarchy_columns(df):\n",
    "    \"\"\"Update DataFrame with proper hierarchy levels\"\"\"\n",
    "    # Extract hierarchy components\n",
    "    hierarchies = df['URL'].apply(extract_url_hierarchy).apply(pd.Series)\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    updated_df = pd.concat([df, hierarchies], axis=1)\n",
    "    \n",
    "    return updated_df\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "\n",
    "# Update hierarchy columns\n",
    "updated_df = update_hierarchy_columns(df)\n",
    "\n",
    "# Save updated file\n",
    "updated_df.to_csv('essex_url_hierarchy_with_dates.csv', index=False)\n",
    "\n",
    "print(\"URL hierarchy levels updated successfully!\")\n",
    "print(f\"Sample entry:\\n{updated_df.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a824e3b4-d870-4cec-8a2c-a24272fd83d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic hierarchy columns created successfully!\n",
      "Created levels: ['level_1', 'level_2', 'level_3', 'level_4', 'level_5', 'level_6', 'level_7', 'level_8']\n",
      "\n",
      "Sample entry:\n",
      "Last Modified                                   2022-11-07\n",
      "URL              https://www.essex.ac.uk/research/showcase\n",
      "Level_1                                           research\n",
      "Level_2                                           showcase\n",
      "Level_3                                                NaN\n",
      "Level_4                                                NaN\n",
      "Level_5                                                NaN\n",
      "Level_6                                                NaN\n",
      "Level_7                                                NaN\n",
      "Level_8                                                NaN\n",
      "level_1                                           research\n",
      "level_2                                           showcase\n",
      "level_3                                               None\n",
      "level_4                                               None\n",
      "level_5                                               None\n",
      "level_6                                               None\n",
      "level_7                                               None\n",
      "level_8                                               None\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def process_url_hierarchy(df):\n",
    "    \"\"\"Dynamically create hierarchy columns based on URL structure\"\"\"\n",
    "    # Remove existing level columns if they exist\n",
    "    level_cols = [col for col in df.columns if col.startswith('level_')]\n",
    "    df = df.drop(columns=level_cols, errors='ignore')\n",
    "    \n",
    "    # Extract path components from URLs\n",
    "    df['parts'] = df['URL'].apply(\n",
    "        lambda x: [p for p in urlparse(x).path.split('/') if p]\n",
    "    )\n",
    "    \n",
    "    # Determine maximum depth across all URLs\n",
    "    max_depth = df['parts'].apply(len).max()\n",
    "    \n",
    "    # Create dynamic level columns\n",
    "    for i in range(max_depth):\n",
    "        df[f'level_{i+1}'] = df['parts'].apply(\n",
    "            lambda x: x[i] if i < len(x) else None\n",
    "        )\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop(columns=['parts'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "\n",
    "# Process URL hierarchy\n",
    "processed_df = process_url_hierarchy(df)\n",
    "\n",
    "# Save updated file\n",
    "processed_df.to_csv('essex_url_hierarchy_with_dates.csv', index=False)\n",
    "\n",
    "print(\"Dynamic hierarchy columns created successfully!\")\n",
    "print(f\"Created levels: {[col for col in processed_df.columns if col.startswith('level_')]}\")\n",
    "print(\"\\nSample entry:\")\n",
    "print(processed_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "938b637d-5155-4dc0-9260-f334b459c820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column cleanup completed successfully!\n",
      "\n",
      "Cleanup verification:\n",
      "Current columns: ['last modified', 'url', 'level_1', 'level_1', 'level_2', 'level_2', 'level_3', 'level_3', 'level_4', 'level_4', 'level_5', 'level_5', 'level_6', 'level_6', 'level_7', 'level_7', 'level_8', 'level_8']\n",
      "\n",
      "Sample hierarchy structure:\n",
      "URL: https://www.essex.ac.uk/courses/UG00001/1/BSc-Accounting\n",
      "level_1: level_1        NaN\n",
      "level_1    courses\n",
      "Name: 8378, dtype: object\n",
      "level_1: level_1        NaN\n",
      "level_1    courses\n",
      "Name: 8378, dtype: object\n",
      "level_2: level_2        NaN\n",
      "level_2    UG00001\n",
      "Name: 8378, dtype: object\n",
      "level_2: level_2        NaN\n",
      "level_2    UG00001\n",
      "Name: 8378, dtype: object\n",
      "level_3: level_3    NaN\n",
      "level_3      1\n",
      "Name: 8378, dtype: object\n",
      "level_3: level_3    NaN\n",
      "level_3      1\n",
      "Name: 8378, dtype: object\n",
      "level_4: level_4               NaN\n",
      "level_4    BSc-Accounting\n",
      "Name: 8378, dtype: object\n",
      "level_4: level_4               NaN\n",
      "level_4    BSc-Accounting\n",
      "Name: 8378, dtype: object\n",
      "level_5: level_5    NaN\n",
      "level_5    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_5: level_5    NaN\n",
      "level_5    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_6: level_6    NaN\n",
      "level_6    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_6: level_6    NaN\n",
      "level_6    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_7: level_7    NaN\n",
      "level_7    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_7: level_7    NaN\n",
      "level_7    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_8: level_8    NaN\n",
      "level_8    NaN\n",
      "Name: 8378, dtype: object\n",
      "level_8: level_8    NaN\n",
      "level_8    NaN\n",
      "Name: 8378, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_duplicate_columns(df):\n",
    "    \"\"\"Remove duplicate columns with case variations and identical data\"\"\"\n",
    "    # Standardize column names to lowercase\n",
    "    df.columns = df.columns.str.lower()\n",
    "    \n",
    "    # Track columns to keep (first occurrence of each duplicate group)\n",
    "    columns_to_keep = []\n",
    "    seen_columns = set()\n",
    "    \n",
    "    # Check columns in reverse order to keep first occurrence\n",
    "    for col in reversed(df.columns):\n",
    "        base_name = col.split('.')[0]  # Handle pandas suffixes\n",
    "        if base_name not in seen_columns:\n",
    "            seen_columns.add(base_name)\n",
    "            columns_to_keep.append(col)\n",
    "    \n",
    "    # Reverse back to original order and select columns\n",
    "    return df[columns_to_keep[::-1]]\n",
    "\n",
    "def verify_cleanup(df):\n",
    "    \"\"\"Check for remaining duplicates and show sample structure\"\"\"\n",
    "    print(\"\\nCleanup verification:\")\n",
    "    print(f\"Current columns: {list(df.columns)}\")\n",
    "    print(\"\\nSample hierarchy structure:\")\n",
    "    sample = df[df['url'].str.contains('/courses/')].iloc[0]\n",
    "    print(f\"URL: {sample['url']}\")\n",
    "    for col in [c for c in df.columns if c.startswith('level_')]:\n",
    "        print(f\"{col}: {sample[col]}\")\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('essex_url_hierarchy_with_dates.csv')\n",
    "\n",
    "# Clean duplicate columns\n",
    "cleaned_df = clean_duplicate_columns(df)\n",
    "\n",
    "# Save cleaned file\n",
    "cleaned_df.to_csv('essex_url_hierarchy_with_dates_cleaned.csv', index=False)\n",
    "\n",
    "# Verify results\n",
    "print(\"Column cleanup completed successfully!\")\n",
    "verify_cleanup(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca9ad10a-f702-4ac9-9fcc-700237ec5ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! Kept columns: ['url', 'last_modified']\n",
      "Total entries preserved: 9063\n",
      "Saved to: essex_url_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_columns(input_file, output_file):\n",
    "    \"\"\"Keep only 'url' and 'last modified' columns\"\"\"\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Standardize column names (case-insensitive and space handling)\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Identify columns to keep\n",
    "    keep_columns = []\n",
    "    if 'url' in df.columns:\n",
    "        keep_columns.append('url')\n",
    "    if 'last_modified' in df.columns:\n",
    "        keep_columns.append('last_modified')\n",
    "    \n",
    "    # Filter columns\n",
    "    if keep_columns:\n",
    "        cleaned_df = df[keep_columns]\n",
    "    else:\n",
    "        raise ValueError(\"No valid columns found to keep\")\n",
    "    \n",
    "    # Save cleaned file\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    return cleaned_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"essex_url_hierarchy_with_dates.csv\"\n",
    "    output_csv = \"essex_url_cleaned.csv\"\n",
    "    \n",
    "    result = clean_columns(input_csv, output_csv)\n",
    "    print(f\"Cleaning complete! Kept columns: {list(result.columns)}\")\n",
    "    print(f\"Total entries preserved: {len(result)}\")\n",
    "    print(f\"Saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f02d6bae-df5d-4f39-aa1f-a6ea42200d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete!\n",
      "Result columns: ['last_modified', 'url', 'level_1', 'level_2', 'level_3', 'level_4', 'level_5', 'level_6', 'level_7', 'level_8']\n",
      "Sample entry:\n",
      "last_modified                                   2022-11-07\n",
      "url              https://www.essex.ac.uk/research/showcase\n",
      "level_1                                           research\n",
      "level_2                                           showcase\n",
      "level_3                                               None\n",
      "level_4                                               None\n",
      "level_5                                               None\n",
      "level_6                                               None\n",
      "level_7                                               None\n",
      "level_8                                               None\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Saved to: essex_url_hierarchy_final.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def process_hierarchy(input_file, output_file):\n",
    "    # Read CSV with proper column handling\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # Standardize column names\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Ensure required columns exist\n",
    "    if 'url' not in df.columns:\n",
    "        raise ValueError(\"URL column missing from dataset\")\n",
    "    \n",
    "    # Create temporary parts column\n",
    "    df['parts'] = df['url'].apply(\n",
    "        lambda x: [p for p in urlparse(x).path.split('/') if p]\n",
    "    )\n",
    "    \n",
    "    # Find maximum URL depth\n",
    "    max_depth = df['parts'].apply(len).max()\n",
    "    \n",
    "    # Create dynamic level columns\n",
    "    for i in range(max_depth):\n",
    "        df[f'level_{i+1}'] = df['parts'].apply(\n",
    "            lambda x: x[i] if i < len(x) else None\n",
    "        )\n",
    "    \n",
    "    # Clean up temporary column\n",
    "    df = df.drop(columns=['parts'])\n",
    "    \n",
    "    # Reorder columns with last_modified first if it exists\n",
    "    column_order = []\n",
    "    if 'last_modified' in df.columns:\n",
    "        column_order.append('last_modified')\n",
    "    column_order += ['url'] + [f'level_{i+1}' for i in range(max_depth)]\n",
    "    \n",
    "    # Final dataframe with sorted columns\n",
    "    final_df = df[column_order]\n",
    "    \n",
    "    # Save to new file\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    return final_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv = \"essex_url_cleaned.csv\"\n",
    "    output_csv = \"essex_url_hierarchy_final.csv\"\n",
    "    \n",
    "    result = process_hierarchy(input_csv, output_csv)\n",
    "    \n",
    "    print(\"Processing complete!\")\n",
    "    print(f\"Result columns: {list(result.columns)}\")\n",
    "    print(f\"Sample entry:\\n{result.iloc[0]}\")\n",
    "    print(f\"\\nSaved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70209835-c786-4e59-a4ef-2287d3f24e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Analysis Report for: essex_url_hierarchy_final.csv\n",
      "==================================================\n",
      "Total URLs: 9063\n",
      "Unique URLs: 9063\n",
      "Duplicate URLs: 0\n",
      "Missing/Empty URLs: 0\n",
      "\n",
      "Sample Duplicate URLs:\n",
      "Series([], )\n",
      "\n",
      "Report saved to: url_analysis_report.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_urls(file_path):\n",
    "    # Read CSV file\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return\n",
    "\n",
    "    # Check for URL column\n",
    "    url_column = None\n",
    "    for col in df.columns:\n",
    "        if col.lower() in ['url', 'urls', 'link']:\n",
    "            url_column = col\n",
    "            break\n",
    "            \n",
    "    if not url_column:\n",
    "        print(\"No URL column found in the file\")\n",
    "        return\n",
    "        \n",
    "    # Get URL statistics\n",
    "    total_urls = len(df)\n",
    "    unique_urls = df[url_column].nunique()\n",
    "    duplicate_count = df.duplicated(subset=[url_column]).sum()\n",
    "    missing_urls = df[url_column].isnull().sum()\n",
    "    \n",
    "    # Print report\n",
    "    print(f\"URL Analysis Report for: {file_path}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total URLs: {total_urls}\")\n",
    "    print(f\"Unique URLs: {unique_urls}\")\n",
    "    print(f\"Duplicate URLs: {duplicate_count}\")\n",
    "    print(f\"Missing/Empty URLs: {missing_urls}\")\n",
    "    print(\"\\nSample Duplicate URLs:\")\n",
    "    print(df[df.duplicated(subset=[url_column], keep=False)][url_column].head(5).to_string(index=False))\n",
    "    \n",
    "    # Save results to text file\n",
    "    with open(\"url_analysis_report.txt\", \"w\") as f:\n",
    "        f.write(f\"URL Analysis Report\\n\")\n",
    "        f.write(f\"File: {file_path}\\n\")\n",
    "        f.write(f\"Total URLs: {total_urls}\\n\")\n",
    "        f.write(f\"Unique URLs: {unique_urls}\\n\")\n",
    "        f.write(f\"Duplicate URLs: {duplicate_count}\\n\")\n",
    "        f.write(f\"Missing/Empty URLs: {missing_urls}\\n\")\n",
    "    \n",
    "    print(\"\\nReport saved to: url_analysis_report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"essex_url_hierarchy_final.csv\"  # Update with your filename\n",
    "    analyze_urls(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "206856be-2d0e-403c-ac57-d431e1259b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL added successfully!\n",
      "\n",
      "New entry structure:\n",
      "     last_modified                                                url  \\\n",
      "9063    2025-05-17  https://www.essex.ac.uk/apprenticeships/health...   \n",
      "\n",
      "              level_1                                 level_2  \n",
      "9063  apprenticeships  health-and-social-care-apprenticeships  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "\n",
    "def add_url_to_hierarchy(file_path, new_url):\n",
    "    # Read existing CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if URL already exists\n",
    "    if new_url in df['url'].values:\n",
    "        print(\"URL already exists in file\")\n",
    "        return df\n",
    "    \n",
    "    # Create new entry\n",
    "    new_entry = {\n",
    "        'last_modified': datetime.today().strftime('%Y-%m-%d'),\n",
    "        'url': new_url\n",
    "    }\n",
    "    \n",
    "    # Parse URL components\n",
    "    parsed = urlparse(new_url)\n",
    "    path_parts = [p for p in parsed.path.split('/') if p]\n",
    "    \n",
    "    # Add level columns\n",
    "    for i, part in enumerate(path_parts, 1):\n",
    "        new_entry[f'level_{i}'] = part\n",
    "    \n",
    "    # Create DataFrame from new entry\n",
    "    new_row = pd.DataFrame([new_entry])\n",
    "    \n",
    "    # Concatenate with existing data\n",
    "    updated_df = pd.concat([df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to match original structure\n",
    "    cols = ['last_modified', 'url'] + [c for c in updated_df.columns if c.startswith('level_')]\n",
    "    updated_df = updated_df[cols]\n",
    "    \n",
    "    # Save updated file\n",
    "    updated_df.to_csv(file_path, index=False)\n",
    "    return updated_df\n",
    "\n",
    "# Configuration\n",
    "file_path = \"essex_url_hierarchy_final.csv\"\n",
    "new_url = \"https://www.essex.ac.uk/apprenticeships/health-and-social-care-apprenticeships\"\n",
    "\n",
    "# Add the new URL\n",
    "updated_df = add_url_to_hierarchy(file_path, new_url)\n",
    "\n",
    "# Show confirmation\n",
    "print(\"URL added successfully!\")\n",
    "print(\"\\nNew entry structure:\")\n",
    "print(updated_df[['last_modified', 'url', 'level_1', 'level_2']].tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ae46d-ae0a-4670-a805-88377764b399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
