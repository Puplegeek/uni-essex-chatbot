{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7af7b555-a1cd-459d-bb1d-db129af83d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: crawl4ai 0.6.2 does not provide the extra 'playwright'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['/opt/anaconda3/bin/python', '-m', 'playwright', 'install', '--with-deps'], returncode=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 📦  Cell 1 – one-time setup\n",
    "#     (installs Crawl4AI, Playwright + its browsers, and helpers)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "%pip install --quiet \"crawl4ai[playwright]\" requests tqdm nest_asyncio\n",
    "\n",
    "import subprocess, sys, nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# install headless Chromium etc. (only the first time on a machine)\n",
    "subprocess.run(\n",
    "    [sys.executable, \"-m\", \"playwright\", \"install\", \"--with-deps\"],\n",
    "    check=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f14d1ef2-dd89-4665-972c-ecb14d9ac155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 🔧 install the missing converter\n",
    "%pip install --quiet markdownify beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3189b63d-13bd-4b54-9b72-1b23328cd56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INIT].... → Crawl4AI 0.6.2\n",
      "[FETCH]... ↓ https://www.essex.ac.uk/courses/PG00742/2/MSc-Data-Science                                           | ✓ | ⏱: 5.11s\n",
      "[SCRAPE].. ◆ https://www.essex.ac.uk/courses/PG00742/2/MSc-Data-Science                                           | ✓ | ⏱: 0.10s\n",
      "[COMPLETE] ● https://www.essex.ac.uk/courses/PG00742/2/MSc-Data-Science                                           | ✓ | ⏱: 5.23s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78eca2583a92428dae8422ef1a46583d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "images:   0%|          | 0/10 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅  Wrote output.md and saved 11 image(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('output.md')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# 🏃  Cell 2 – scrape an Essex course page to Markdown (cookie-free)\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import asyncio, os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin, urlsplit\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
    "# → If you prefer pure-HTTP (no browser) crawling, swap these lines:\n",
    "# from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy\n",
    "\n",
    "# ─────────── helpers ────────────────────────────────────────────\n",
    "def _safe_name(u: str, idx: int) -> str:\n",
    "    \"\"\"Return a numbered filename preserving the URL’s extension.\"\"\"\n",
    "    ext = os.path.splitext(urlsplit(u).path)[1] or \".jpg\"\n",
    "    return f\"{idx:03d}{ext}\"\n",
    "\n",
    "COOKIE_SELECTORS = [\n",
    "    \"#onetrust-banner-sdk\",\n",
    "    \"#onetrust-consent-sdk\",\n",
    "    \".cookie-bar\",\n",
    "    \".ot-sdk-container\",\n",
    "]\n",
    "\n",
    "async def scrape_to_markdown(\n",
    "    url: str,\n",
    "    out_md: Path = Path(\"output.md\"),\n",
    "    assets_dir: Path = Path(\"assets\"),\n",
    "):\n",
    "    \"\"\"Fetch `url`, clean it, download images, write Markdown.\"\"\"\n",
    "    run_cfg = CrawlerRunConfig(\n",
    "        css_selector=\"#content\",                 # Essex main wrapper\n",
    "        wait_for_images=True,\n",
    "        remove_overlay_elements=True,\n",
    "        js_code=\"\"\"(() => {\n",
    "            const btn = document.querySelector('#onetrust-accept-btn-handler');\n",
    "            if (btn) btn.click();\n",
    "        })();\"\"\",                               # accept cookies\n",
    "        markdown_generator=DefaultMarkdownGenerator(\n",
    "            options={\"ignore_links\": False, \"ignore_images\": False}\n",
    "        ),\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy()) as crawler:\n",
    "    async with AsyncWebCrawler() as crawler:    # Playwright default\n",
    "        result = await crawler.arun(url, config=run_cfg)\n",
    "\n",
    "    if not result.success:\n",
    "        raise RuntimeError(result.error_message)\n",
    "\n",
    "    # —— clean HTML ——————————————————————————\n",
    "    html_content = result.html                 # str in crawl4ai 0.6.2\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    for sel in COOKIE_SELECTORS:\n",
    "        for node in soup.select(sel):\n",
    "            node.decompose()                   # remove banner nodes\n",
    "\n",
    "    # —— download images & relink ——————————\n",
    "    assets_dir.mkdir(exist_ok=True)\n",
    "    session = requests.Session()\n",
    "\n",
    "    for idx, img in enumerate(\n",
    "        tqdm(soup.find_all(\"img\"), desc=\"images\", unit=\"img\")\n",
    "    ):\n",
    "        src_attr = img.get(\"data-src\") or img.get(\"src\")\n",
    "        if not src_attr:\n",
    "            continue\n",
    "        full_url = urljoin(url, src_attr)      # make absolute\n",
    "        try:\n",
    "            r = session.get(full_url, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            fname = _safe_name(full_url, idx)\n",
    "            local_path = assets_dir / fname\n",
    "            local_path.write_bytes(r.content)\n",
    "            img[\"src\"] = f\"{assets_dir}/{fname}\"   # point Markdown here\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ cannot fetch {full_url}: {e}\")\n",
    "\n",
    "    # —— HTML → Markdown ————————————————————\n",
    "    markdown = md(str(soup), heading_style=\"ATX\", strip=[\"script\", \"style\"])\n",
    "    out_md.write_text(markdown, encoding=\"utf-8\")\n",
    "    print(f\"\\n✅  Wrote {out_md} and saved {len(list(assets_dir.glob('*')))} image(s).\")\n",
    "    return out_md\n",
    "\n",
    "\n",
    "# ————————————————————————————————————————————————\n",
    "# CHANGE ONLY THIS URL BETWEEN RUNS\n",
    "url = \"https://www.essex.ac.uk/courses/PG00742/2/MSc-Data-Science\"\n",
    "# ————————————————————————————————————————————————\n",
    "\n",
    "await scrape_to_markdown(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32385f6e-2603-48f3-86eb-3ef37dd0e782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Wrote output_clean.md (440 lines). Preview ↓\n",
      "\n",
      "## Overview\n",
      "\n",
      "The details\n",
      "\n",
      "Course:  Data Science with Professional Placement\n",
      "\n",
      "Start date:  October 2025\n",
      "\n",
      "Study mode:  Full-time\n",
      "\n",
      "Duration:  2 years\n",
      "\n",
      "Location:  Colchester Campus\n",
      "\n",
      "Based in:  [Mathematics, Statistics and Actuarial Science (School of)](https://www.essex.ac.uk/departments/mathematics-statistics-and-actuarial-science)\n",
      "\n",
      "The techniques we use to model and manipulate data guide the political, financial, and social decisions that shape our modern society and are the basis of economy growth and business success. Technology is growing and evolving at an incredible speed, and the growth rate of both the data we generate and the devices we use to process it can only increase.\n",
      "\n",
      "Data science is a growing and important field of study with an increasing number of jobs and opportunities within both the private and public sectors. The application of theory and methods to real-world problems is at the core of data science, which aims especially to use and exploit big data.\n",
      "\n",
      "If you are interested in solving real-world problems, like to develop skills to use smart devices efficiently, want to use and to foster your understanding of mathematics, and keen to use statistical techniques and methods to interpret data, MSc Data Science at Essex is for you. You study a balance of solid theory and practical application, including:\n",
      "\n",
      "* computer science\n",
      "* programming\n",
      "* statistics\n",
      "* data analysis\n",
      "* probability\n",
      "\n",
      "A successful career in data science requires you to possess truly interdisciplinary knowledge, so we ensure that you graduate with a wide-ranging yet specialised set of skills in this area. You are taught mainly within our [School of Mathematics, Statistics and Actuarial Science](https://www.essex.ac.uk/departments/mathematics-statistics-and-actuarial-science) and our [School of Computer Science and Electronic Engineering](https://www.essex.ac.uk/departments/computer-science-and-electronic-engineering), but also benefit from input from our [Essex Business School](https://www.essex.ac.uk/departments/essex-business-school) and our [Essex Pathways Department](https://www.essex.ac.uk/departments/essex-pathways). Data scientists are required in every sector, carrying out statistical analysis or mining data on social media, so our course can open the door to almost any industry, from health, to government, to publishing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 📄  Trim everything above \"## Overview\" in output.md\n",
    "from pathlib import Path, PurePosixPath\n",
    "\n",
    "src  = Path(\"output.md\")          # original file\n",
    "dest = Path(\"output_clean.md\")    # cleaned file\n",
    "\n",
    "if not src.exists():\n",
    "    raise FileNotFoundError(\"output.md not found – run the scraper first.\")\n",
    "\n",
    "lines = src.read_text(encoding=\"utf-8\").splitlines()\n",
    "try:\n",
    "    start = next(i for i, ln in enumerate(lines) if ln.lstrip().startswith(\"## Overview\"))\n",
    "except StopIteration:\n",
    "    raise ValueError(\"'## Overview' header not found in output.md\")\n",
    "\n",
    "dest.write_text(\"\\n\".join(lines[start:]), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅  Wrote {dest} ({len(lines) - start} lines). Preview ↓\\n\")\n",
    "print(\"\\n\".join(lines[start:start+30]))   # first 30 lines of the cleaned file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5ae0851-21e6-4879-903b-18f084ebf464",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Heading '## Overview' not found in /Users/syedakash/Library/Jupyter/runtime/kernel-db93c9b4-4552-4dd5-98d1-dfed05b4298a.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 28\u001b[0m, in \u001b[0;36mtrim_above_anchor\u001b[0;34m(src, dest, anchor)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, ln \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines)\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m ln\u001b[38;5;241m.\u001b[39mlstrip()\u001b[38;5;241m.\u001b[39mstartswith(anchor))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m     parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdest\u001b[39m\u001b[38;5;124m\"\u001b[39m, nargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, default\u001b[38;5;241m=\u001b[39mDEFAULT_DEST)\n\u001b[1;32m     41\u001b[0m     args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args()                \u001b[38;5;66;03m# eat unknown opts\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     trim_above_anchor(Path(args\u001b[38;5;241m.\u001b[39msrc), Path(args\u001b[38;5;241m.\u001b[39mdest), ANCHOR)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# running inside an imported cell (no CLI args) – do the default trim\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     trim_above_anchor(DEFAULT_SRC, DEFAULT_DEST, ANCHOR)\n",
      "Cell \u001b[0;32mIn[19], line 31\u001b[0m, in \u001b[0;36mtrim_above_anchor\u001b[0;34m(src, dest, anchor)\u001b[0m\n\u001b[1;32m     28\u001b[0m     start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i, ln \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lines)\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m ln\u001b[38;5;241m.\u001b[39mlstrip()\u001b[38;5;241m.\u001b[39mstartswith(anchor))\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeading \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00manchor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msrc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m dest\u001b[38;5;241m.\u001b[39mwrite_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lines[start_idx:]), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅  Wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lines)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lines kept, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m removed).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Heading '## Overview' not found in /Users/syedakash/Library/Jupyter/runtime/kernel-db93c9b4-4552-4dd5-98d1-dfed05b4298a.json"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "trim_overview.py\n",
    "~~~~~~~~~~~~~~~~\n",
    "Cut everything ABOVE the first \"## Overview\" heading in a Markdown file.\n",
    "\n",
    "• Jupyter use:\n",
    "      # just run the cell; it uses output.md → output_clean.md\n",
    "• CLI use:\n",
    "      python trim_overview.py input.md output_clean.md\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "# ---------- defaults for notebook use ---------------------------------\n",
    "DEFAULT_SRC  = Path(\"output.md\")\n",
    "DEFAULT_DEST = Path(\"output_clean.md\")\n",
    "ANCHOR       = \"## Overview\"\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "def trim_above_anchor(src: Path, dest: Path, anchor: str = ANCHOR) -> None:\n",
    "    if not src.exists():\n",
    "        raise FileNotFoundError(f\"{src} not found – did the scraper run?\")\n",
    "    lines = src.read_text(encoding=\"utf-8\").splitlines()\n",
    "    try:\n",
    "        start_idx = next(i for i, ln in enumerate(lines)\n",
    "                         if ln.lstrip().startswith(anchor))\n",
    "    except StopIteration:\n",
    "        raise ValueError(f\"Heading '{anchor}' not found in {src}\")\n",
    "    dest.write_text(\"\\n\".join(lines[start_idx:]), encoding=\"utf-8\")\n",
    "    print(f\"✅  Wrote {dest} \"\n",
    "          f\"({len(lines) - start_idx} lines kept, {start_idx} removed).\")\n",
    "\n",
    "# ---------- run --------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(add_help=False)   # ignore -f etc.\n",
    "    parser.add_argument(\"src\",  nargs=\"?\", default=DEFAULT_SRC)\n",
    "    parser.add_argument(\"dest\", nargs=\"?\", default=DEFAULT_DEST)\n",
    "    args, _ = parser.parse_known_args()                # eat unknown opts\n",
    "    trim_above_anchor(Path(args.src), Path(args.dest), ANCHOR)\n",
    "else:\n",
    "    # running inside an imported cell (no CLI args) – do the default trim\n",
    "    trim_above_anchor(DEFAULT_SRC, DEFAULT_DEST, ANCHOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9c60f0-dd86-4422-ab89-659eae94111c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
