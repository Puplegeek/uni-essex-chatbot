# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸ“¦  Cell 1 â€“ one-time setup
#     (installs Crawl4AI, Playwright + its browsers, and helpers)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
%pip install --quiet "crawl4ai[playwright]" requests tqdm nest_asyncio

import subprocess, sys, nest_asyncio
nest_asyncio.apply()

# install headless Chromium etc. (only the first time on a machine)
subprocess.run(
    [sys.executable, "-m", "playwright", "install", "--with-deps"],
    check=True,
)



# ðŸ”§ install the missing converter
%pip install --quiet markdownify beautifulsoup4



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ðŸƒ  Cell 2 â€“ scrape an Essex course page to Markdown (cookie-free)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import asyncio, os
from pathlib import Path
from urllib.parse import urljoin, urlsplit

import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from tqdm.auto import tqdm

from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
# â†’ If you prefer pure-HTTP (no browser) crawling, swap these lines:
# from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _safe_name(u: str, idx: int) -> str:
    """Return a numbered filename preserving the URLâ€™s extension."""
    ext = os.path.splitext(urlsplit(u).path)[1] or ".jpg"
    return f"{idx:03d}{ext}"

COOKIE_SELECTORS = [
    "#onetrust-banner-sdk",
    "#onetrust-consent-sdk",
    ".cookie-bar",
    ".ot-sdk-container",
]

async def scrape_to_markdown(
    url: str,
    out_md: Path = Path("output.md"),
    assets_dir: Path = Path("assets"),
):
    """Fetch `url`, clean it, download images, write Markdown."""
    run_cfg = CrawlerRunConfig(
        css_selector="#content",                 # Essex main wrapper
        wait_for_images=True,
        remove_overlay_elements=True,
        js_code="""(() => {
            const btn = document.querySelector('#onetrust-accept-btn-handler');
            if (btn) btn.click();
        })();""",                               # accept cookies
        markdown_generator=DefaultMarkdownGenerator(
            options={"ignore_links": False, "ignore_images": False}
        ),
        verbose=True,
    )

    # async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy()) as crawler:
    async with AsyncWebCrawler() as crawler:    # Playwright default
        result = await crawler.arun(url, config=run_cfg)

    if not result.success:
        raise RuntimeError(result.error_message)

    # â€”â€” clean HTML â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    html_content = result.html                 # str in crawl4ai 0.6.2
    soup = BeautifulSoup(html_content, "html.parser")

    for sel in COOKIE_SELECTORS:
        for node in soup.select(sel):
            node.decompose()                   # remove banner nodes

    # â€”â€” download images & relink â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    assets_dir.mkdir(exist_ok=True)
    session = requests.Session()

    for idx, img in enumerate(
        tqdm(soup.find_all("img"), desc="images", unit="img")
    ):
        src_attr = img.get("data-src") or img.get("src")
        if not src_attr:
            continue
        full_url = urljoin(url, src_attr)      # make absolute
        try:
            r = session.get(full_url, timeout=30)
            r.raise_for_status()
            fname = _safe_name(full_url, idx)
            local_path = assets_dir / fname
            local_path.write_bytes(r.content)
            img["src"] = f"{assets_dir}/{fname}"   # point Markdown here
        except Exception as e:
            print(f"âš  cannot fetch {full_url}: {e}")

    # â€”â€” HTML â†’ Markdown â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
    markdown = md(str(soup), heading_style="ATX", strip=["script", "style"])
    out_md.write_text(markdown, encoding="utf-8")
    print(f"\nâœ…  Wrote {out_md} and saved {len(list(assets_dir.glob('*')))} image(s).")
    return out_md


# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# CHANGE ONLY THIS URL BETWEEN RUNS
url = "https://www.essex.ac.uk/courses/PG00742/2/MSc-Data-Science"
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

await scrape_to_markdown(url)



# ðŸ“„  Trim everything above "## Overview" in output.md
from pathlib import Path, PurePosixPath

src  = Path("output.md")          # original file
dest = Path("output_clean.md")    # cleaned file

if not src.exists():
    raise FileNotFoundError("output.md not found â€“ run the scraper first.")

lines = src.read_text(encoding="utf-8").splitlines()
try:
    start = next(i for i, ln in enumerate(lines) if ln.lstrip().startswith("## Overview"))
except StopIteration:
    raise ValueError("'## Overview' header not found in output.md")

dest.write_text("\n".join(lines[start:]), encoding="utf-8")

print(f"âœ…  Wrote {dest} ({len(lines) - start} lines). Preview â†“\n")
print("\n".join(lines[start:start+30]))   # first 30 lines of the cleaned file



#!/usr/bin/env python3
"""
trim_overview.py
~~~~~~~~~~~~~~~~
Cut everything ABOVE the first "## Overview" heading in a Markdown file.

â€¢ Jupyter use:
      # just run the cell; it uses output.md â†’ output_clean.md
â€¢ CLI use:
      python trim_overview.py input.md output_clean.md
"""

from pathlib import Path
import argparse
import sys

# ---------- defaults for notebook use ---------------------------------
DEFAULT_SRC  = Path("output.md")
DEFAULT_DEST = Path("output_clean.md")
ANCHOR       = "## Overview"
# ----------------------------------------------------------------------

def trim_above_anchor(src: Path, dest: Path, anchor: str = ANCHOR) -> None:
    if not src.exists():
        raise FileNotFoundError(f"{src} not found â€“ did the scraper run?")
    lines = src.read_text(encoding="utf-8").splitlines()
    try:
        start_idx = next(i for i, ln in enumerate(lines)
                         if ln.lstrip().startswith(anchor))
    except StopIteration:
        raise ValueError(f"Heading '{anchor}' not found in {src}")
    dest.write_text("\n".join(lines[start_idx:]), encoding="utf-8")
    print(f"âœ…  Wrote {dest} "
          f"({len(lines) - start_idx} lines kept, {start_idx} removed).")

# ---------- run --------------------------------------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(add_help=False)   # ignore -f etc.
    parser.add_argument("src",  nargs="?", default=DEFAULT_SRC)
    parser.add_argument("dest", nargs="?", default=DEFAULT_DEST)
    args, _ = parser.parse_known_args()                # eat unknown opts
    trim_above_anchor(Path(args.src), Path(args.dest), ANCHOR)
else:
    # running inside an imported cell (no CLI args) â€“ do the default trim
    trim_above_anchor(DEFAULT_SRC, DEFAULT_DEST, ANCHOR)




