import requests
import xml.etree.ElementTree as ET
import pandas as pd

# Sitemap URL
sitemap_url = 'https://www.essex.ac.uk/content.xml'

# Fetch and decode
response = requests.get(sitemap_url)
content = response.content.decode('utf-8', errors='replace')

# Parse XML
try:
    root = ET.fromstring(content)
    ns = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}

    # Extract URLs and lastmod
    data = []
    for url_elem in root.findall('ns:url', ns):
        loc = url_elem.find('ns:loc', ns)
        lastmod = url_elem.find('ns:lastmod', ns)
        data.append({
            'URL': loc.text if loc is not None else '',
            'Last Modified': lastmod.text if lastmod is not None else ''
        })

    df = pd.DataFrame(data)
    print(f"✅ Extracted {len(df)} URLs.")
    print(df.head())

    df.to_csv("essex_content_sitemap.csv", index=False)
    print("Saved to 'essex_content_sitemap.csv'")

except ET.ParseError as e:
    print("❌ XML Parse Error:", e)



# Extract the second path segment after the domain
df['Second Prefix'] = df['URL'].apply(lambda url: url.replace('https://www.essex.ac.uk/', '').split('/')[0])

# Show unique second prefixes
unique_prefixes = df['Second Prefix'].unique()
print(f"✅ Found {len(unique_prefixes)} unique second prefixes:")
print(sorted(unique_prefixes))



prefix_counts = df['Second Prefix'].value_counts()
print("✅ Frequency of each second prefix:")
print(prefix_counts)



!pip install selenium webdriver-manager


from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import time

# Initialize Chrome driver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
url = 'https://www.essex.ac.uk/subjects/computer-science'
driver.get(url)

# Handle cookie consent
try:
    cookie_accept = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.ID, "onetrust-accept-btn-handler"))
    )
    cookie_accept.click()
    print("Accepted cookies")
except Exception as e:
    print("No cookie popup found or could not accept cookies")

# List to store all course URLs
all_course_urls = []

# Tab IDs for Undergraduate, Masters, and Research
tab_ids = ['ug', 'pg', 'pgr']

try:
    for tab_id in tab_ids:
        # Switch to current tab
        tab = WebDriverWait(driver, 20).until(
            EC.element_to_be_clickable((By.CSS_SELECTOR, f"a[href='#{tab_id}']"))
        )
        driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", tab)
        driver.execute_script("arguments[0].click();", tab)
        print(f"Switched to {tab_id.upper()} tab")
        time.sleep(2)  # Allow tab content to load
        
        # Handle "Load More" button
        while True:
            try:
                # Find and click "Load More" using JavaScript
                load_more = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, "#load-more .button"))
                )
                driver.execute_script("arguments[0].scrollIntoView({behavior: 'smooth', block: 'center'});", load_more)
                driver.execute_script("arguments[0].click();", load_more)
                print("Clicked Load More button")
                time.sleep(2)  # Wait for new content to load
            except (NoSuchElementException, TimeoutException):
                print("No more 'Load More' button or reached end")
                break
        
        # Collect all course links
        course_cards = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "a.course-search-card"))
        )
        tab_urls = [card.get_attribute("href") for card in course_cards]
        all_course_urls.extend(tab_urls)
        print(f"Found {len(tab_urls)} courses in {tab_id.upper()} tab")

except Exception as e:
    print(f"Error occurred: {str(e)}")
finally:
    driver.quit()

# Remove duplicates and print results
unique_urls = list(set(all_course_urls))
print(f"\nTotal unique courses found: {len(unique_urls)}")
for url in unique_urls:
    print(url)



